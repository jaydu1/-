{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯(Naive Bayes)文本分类算法——杜金鸿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一.定义\n",
    "（1）设$x=\\{a_1,\\cdots,a_m\\}$为一个待分类项，而每个$a_i$为$x$的一个特征属性;\n",
    "\n",
    "（2）有类别集合$C=\\{y_1,\\cdots,y_n\\}$;\n",
    "\n",
    "（3）计算$P(y_1|x),\\cdots,P(y_n|x)$;\n",
    "\n",
    "（4）若$P(y_k|x)=\\max\\{P(y_1|x),\\cdots,P(y_n|x)\\}$，则$x\\in y_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二.条件概率的计算\n",
    "（1）得到一个已知分类的待分类集合，即训练集；\n",
    "\n",
    "（2）统计各类别下的各特征属性，用频率估计概率，得到条件概率估计，即$$P(a_1|y_1),P(a_2|y_1),\\cdots,P(a_m|y_1)$$\n",
    "$$P(a_1|y_2),P(a_2|y_2),\\cdots,P(a_m|y_2)$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$P(a_1|y_n),P(a_2|y_n),\\cdots,P(a_m|y_n);$$\n",
    "\n",
    "（3）假设个特征属性相互独立，则它们条件独立，则根据贝叶斯公式有$$P(y_i|x)=\\frac{P(x|y_i)P(y_i)}{P(x)}$$\n",
    "    因为右端分母对于选定的$y_1,\\cdots,y_n$为常数，只要将分子最大化即可，又有各特征属性相互独立，所以有$$P(x|y_i)P(y_i)=P(a_1|y_i)P(a_2|y_i)\\cdots P(a_m|y_i)=P(y_i)\\prod\\limits_{i=1}^m a_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三.具体流程\n",
    "（1）从训练集抽取数据，分词、去听用词后，统计不同单词，构建总单词集合，即所有样本（文档）中有效单词的集合；\n",
    "\n",
    "（2）将单词集合中的单词转换为向量，采取TF-IDF权重策略，构建向量空间模型：\n",
    "\n",
    "·   文档集合：$D$\n",
    "\n",
    "   ·   文档数：$|D|$\n",
    "\n",
    "   ·   词数（Term Count）：$n_{i,j}$，第个$i$样本（文档）内所含第$j$个单词的数量；\n",
    "\n",
    "   ·   词频（Term Frequency,TF），是对词数的归一化，体现了各单词在某一特定文档的重要性：$$TF_{i,j}=\\frac{n_{i,j}}{\\sum\\limits_{k}n_{k,j}}$$\n",
    "\n",
    "   ·   逆向文档频率（Inverse Document Frequency,IDF），体现一个单词普遍重要性：$$IDF_i=\\log\\frac{|D|}{|\\{j:t_i\\in d_j\\}|}$$\n",
    "\n",
    "   ·   词频逆文档频率（TFIDF），某一特定文件内的高单词频率以及该词语在文档集合中的低文件频率，乘积较大，因此TF\\_IDF倾向于过滤掉常见的词语，保留重要词语：\n",
    "   $$TFIDF_{i,j}=TF_i\\times IDF_{i,j}$$\n",
    "\n",
    "（2）对每个类别计算$P(y_i)$;\n",
    "\n",
    "（3）对每个特征属性计算所有划分的条件概率$P(a_j|y_i)$；\n",
    "\n",
    "（4）将测试集文档转换为词向量，并映射到训练集词典，即用训练集词典中的词向量表示测试集文档；\n",
    "\n",
    "（5）每一个测试集文档$x$，对每个类别计算$P(x|y_i)P(y_i)$;\n",
    "\n",
    "（5）求$\\max\\limits_i\\{ P(x|y_i)P(y_i)\\}$所对应的类别$i$，即是$x$的预测分类."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四.代码实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.创建简单外部数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my','dog','has','flea','problems','help','please'],\n",
    "                 ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                 ['my','dalmation','is','so','cute','I','love','him','my'],\n",
    "                 ['stop','posting','stupid','worthless','garbage'],\n",
    "                 ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                 ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec=[0,1,0,1,0,1]\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.创建贝叶斯类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NBayes(object):\n",
    "    def __init__(self):\n",
    "        self.vocabulary=[]      # 词典\n",
    "        self.idf=0              # 词典的IDF权值向量\n",
    "        self.tf=0               # 训练集的权值矩阵\n",
    "        self.x_y_prob=0         # P(x|yi)\n",
    "        self.y_prob={}          # 类别词典 P(yi)\n",
    "        self.labels=[]          # \n",
    "        self.doc_len=0          # 训练集文档数\n",
    "        self.dict_len=0         # 词典内单词数\n",
    "        self.testset=0          # 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）训练集词向量生成函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def train_set(self,trainset,classVec):\n",
    "        self.cate_prob(classVec)                            # 计算P(yi)\n",
    "        self.doc_len=len(trainset)\n",
    "        tempset=set()\n",
    "        [tempset.add(word) for doc in trainset for word in doc] # 生成词典\n",
    "        self.vocabulary=list(tempset)\n",
    "        self.dict_len=len(self.vocabulary)\n",
    "        self.calc_tfidf(trainset)                           # 计算词频数据集\n",
    "        self.build_x_y_prob()                               # 计算P(x|yi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）cate_prob 函数：计算$P(y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def cate_prob(self,classVec):\n",
    "        self.labels=classVec\n",
    "        labeltemps=set(self.labels)\n",
    "        for labeltemp in labeltemps:\n",
    "            # 统计列表中的重复分类 self.labels.count(labeltemp)\n",
    "            self.y_prob[labeltemp]=float(self.labels.count(labeltemp)\n",
    "                                        )/float(len(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）calc_tfidf 函数：生成词频向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def calc_tfidf(self,trainset):\n",
    "        self.idf=np.zeros([1,self.dict_len])            # 1*词典数\n",
    "        self.tf=np.zeros([self.doc_len,self.dict_len])  # 训练集文档数*词典数\n",
    "        for index in xrange(self.doc_len):\n",
    "            for word in trainset[index]:\n",
    "                self.tf[index,self.vocabulary.index(word)]+=1\n",
    "            self.tf[index]=self.tf[index]/float(len(trainset[index]))\n",
    "            for signleword in set(trainset[index]):\n",
    "                self.idf[0,self.vocabulary.index(signleword)]+=1\n",
    "        self.idf=np.log(float(self.doc_len)/self.idf)\n",
    "        self.id=np.multiply(self.tf,self.idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（5）build_tdm 函数：构建向量空间$P(x|y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def build_tdm(self):\n",
    "        self.tdm=np.zeros([len(self.Pcates),self.vocablen]) # 类别行*词典列\n",
    "        sumlist=np.zeros([len(self.Pcates),1])               # 每个分类总数\n",
    "        for index in xrange(self.doclength):\n",
    "            self.tdm[self.labels[index]]+=self.tf[index]\n",
    "            sumlist[self.labels[index]]=np.sum(self.tdm[self.labels[index]])\n",
    "        self.tdm=self.tdm/sumlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（6）map2vocabulary 函数：将测试集的词向量映射到当前词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def map2vocalurary(self,testdata):\n",
    "        self.testset=np.zeros([1,self.dict_len])\n",
    "        for word in testdata:\n",
    "            self.testset[0,self.vocabulary.index(word)]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（7）predict 函数：预测测试文档分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def predict(self,testset):\n",
    "        # 判断测试文档词典长度与训练集词典长度是否相等\n",
    "        if np.shape(testset)[1]!=self.dict_len:\n",
    "            print \"输入错误\"\n",
    "            exit(0)\n",
    "        predvalue=0\n",
    "        predclass=\"\"\n",
    "        for x_y_prob_vect,keyclass in zip(self.x_y_prob,self.y_prob):\n",
    "            temp=np.sum(testset*x_y_prob_vect*self.y_prob[keyclass])\n",
    "            if temp>predvalue:\n",
    "                predvalue=temp\n",
    "                predclass=keyclass\n",
    "        return predclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataSet,listClasses=loadDataSet()\n",
    "nb=NBayes()\n",
    "nb.train_set(dataSet,listClasses)\n",
    "nb.map2vocabulary(dataSet[0])\n",
    "print nb.predict(nb.testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 参考文献：\n",
    "【1】http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "【2】《机器学习算法原理与编程实践》——郑捷"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
